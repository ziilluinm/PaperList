Att机制<br>
【经典attention在seq2seq】<br>
paper传送门：<br>
<br>
代码传送门:<br>
<br>

【self-attention】Attention Is All You Need<br>
paper传送门：
https://arxiv.org/abs/1706.03762<br>
代码传送门:
https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py<br>

声纹识别<br>
【ge2e】generalized end-to-end loss for speaker verification <br>
paper传送门：
https://arxiv.org/abs/1710.10467<br>
代码传送门:
https://github.com/Janghyun1230/Speaker_Verification<br>

【ge2e+att】<br>
paper传送门：
<br>
代码传送门:
https://github.com/liyongze/lstm_speaker_verification<br>

【crnn+att】Seq2Seq Attentional Siamese Neural Networks for Text-dependent Speaker Verification<br>
paper传送门：
https://pan.baidu.com/s/1fXnqrsiL9RrEb3BLbQiSyw<br>

【ENAS】Efficient Neural Architecture Search via Parameters Sharing<br>
paper传送门：
https://arxiv.org/abs/1802.03268<br>
代码传送门:
https://github.com/carpedm20/ENAS-pytorch<br>
https://github.com/melodyguan/enas

【NAS综述】Neural Architecture Search: A Survey<br>
paper传送门：
http://jmlr.org/papers/volume20/18-598/18-598.pdf<br>



